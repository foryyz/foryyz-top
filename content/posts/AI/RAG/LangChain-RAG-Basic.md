---
title: 'LangChain-RAG - 1.åŸºç¡€ç¨‹åºå’Œæ¦‚å¿µè¯´æ˜'
date: 2024-11-04T17:22:47+08:00
draft: false
tags: ['AI','LangChain','RAG']
---

# LangChain - RAG

åœ¨æ‰§è¡Œä»£ç ä¹‹å‰ä½ éœ€è¦å‡†å¤‡å¥½pythonç¯å¢ƒ:  [Click Me!](#00-pythonç¯å¢ƒ)

## 1 ä½¿ç”¨LangChainåˆ›å»ºä¸€ä¸ªç®€å• LLM åº”ç”¨ç¨‹åº

[ç‚¹å‡»æŸ¥çœ‹æ•ˆæœå›¾](#tp-image-playground)

ä»¥**openai**çš„gpt-3.5-turboä¸ºä¾‹

[Introduction | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/docs/introduction/)

### 1.1 Using Language Models

ä¸‹é¢ç¤ºä¾‹ç›´æ¥è¿æ¥äº†openaiçš„å¤§æ¨¡å‹ï¼Œæ›´å¤šï¼š[Modelsçš„å‚æ•°: base_url, api_keyç­‰](https://python.langchain.com/docs/concepts/chat_models/#standard-parameters)

```python
import os
os.environ["OPENAI_API_KEY"] = "sk-xxxx"

from langchain_openai import ChatOpenAI
# æ›´å¤šå‚æ•°æŸ¥çœ‹ä¸Šé¢çš„é“¾æ¥
model = ChatOpenAI(model="gpt-3.5-turbo",api_key="sk-xxxx", base_url="https://api.bianxie.ai/v1")
```

**ChatModel** æ˜¯ LangChain â€œRunnablesâ€ çš„å®ä¾‹ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å…¬å¼€äº†ä¸€ä¸ªç”¨äºä¸å®ƒä»¬äº¤äº’çš„æ ‡å‡†æ¥å£ã€‚è¦ç®€å•åœ°è°ƒç”¨æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¶ˆæ¯åˆ—è¡¨ä¼ é€’ç»™ .invoke æ–¹æ³•

- [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html)
- [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

```python
from langchain_core.messages import HumanMessage, SystemMessage
messages = [
    # SystemMessage æç¤ºå¤§æ¨¡å‹ï¼Œå°†æ¥æ”¶åˆ°çš„æ–‡å­—ç¿»è¯‘æˆä¸­æ–‡
    SystemMessage(content="Translate the following from English into Chinese"),
    # HumanMessage è¡¨ç¤ºè¦ç¿»è¯‘çš„æ–‡å­—
    HumanMessage(content="hi!"),
]
model.invoke(messages)
```

æŸ¥çœ‹æ›´å¤šï¼š[Message](https://python.langchain.com/api_reference/core/messages.html)

æ¥è‡ªæ¨¡å‹çš„å“åº”ä¹Ÿå°±æ˜¯`model.invoke(messages)`ä¸º â†“

```
content='ä½ å¥½ï¼' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-4ebdace1-b84f-45ce-a986-71dac7e2c0f6-0' usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}
```

å®ƒçš„ç±»å‹ä¸ºï¼š**AIMessage**

### 1.2 OutputParsers - è¾“å‡ºè§£é‡Šå™¨

æ¥è‡ªæ¨¡å‹çš„å“åº”æ˜¯ **AIMessage**ã€‚è¿™åŒ…å«å­—ç¬¦ä¸²å“åº”ä»¥åŠæœ‰å…³å“åº”çš„å…¶ä»–å…ƒæ•°æ®ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬å¯èƒ½åªæƒ³å¤„ç†å­—ç¬¦ä¸² responseã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç®€å•çš„è¾“å‡ºè§£æå™¨æ¥è§£ææ­¤å“åº”ã€‚

```python
from langchain_core.output_parsers import StrOutputParser
parser = StrOutputParser()

# æ–¹å¼1.å•ç‹¬ä½¿ç”¨
result = model.invoke(messages)
parser.invoke(result)

# æ–¹å¼2.ç”¨ è¾“å‡ºè§£é‡Šå™¨ æ¥ â€œé“¾æ¥â€ æ¨¡å‹ã€‚è¿™æ„å‘³ç€è¿™ä¸ª output parser å°†åœ¨æ­¤é“¾ä¸­æ¯æ¬¡è¢«è°ƒç”¨ã€‚æ­¤é“¾é‡‡ç”¨è¯­è¨€æ¨¡å‹çš„ input ç±»å‹ï¼ˆå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨ï¼‰ï¼Œå¹¶è¿”å›è¾“å‡ºè§£æå™¨çš„è¾“å‡ºç±»å‹ï¼ˆstringï¼‰ã€‚
chain = model | parser # é“¾æ¥
chain.invoke(messages)
```

### 1.3 Prompt Templates - æç¤ºè¯æ¨¡æ¿

[Prompt Templates](https://python.langchain.com/docs/tutorials/llm_chain/#prompt-templates)

è¿™ä¸ªç¤ºä¾‹æ¥å—ä¸¤ä¸ªç”¨æˆ·å˜é‡:

- `language`: The language to translate text into
- `text`: The text to translate

[èŠå¤©æç¤ºè¯æ¨¡æ¿](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```python
from langchain_core.prompts import ChatPromptTemplate
system_template = "Translate the following into {language}:"

# åˆ›å»º PromptTemplate
prompt_template = ChatPromptTemplate.from_messages(
    [("system", system_template), ("user", "{text}")]
)
```

æ­¤æç¤ºæ¨¡æ¿çš„è¾“å…¥æ˜¯å­—å…¸ã€‚æˆ‘ä»¬å¯ä»¥å•ç‹¬ä½¿ç”¨è¿™ä¸ªæç¤ºæ¨¡æ¿ï¼Œ**çœ‹çœ‹å®ƒè‡ªå·±åšäº†ä»€ä¹ˆ**

```python
result = prompt_template.invoke({"language": "Chinese", "text": "hi"})
print(result)
>>>
ChatPromptValue(messages=[SystemMessage(content='Translate the following into Chinese:'), HumanMessage(content='hi')]) # å¯ä»¥çœ‹åˆ°å®ƒè¿”å›ä¸€ä¸ªç”±ä¸¤æ¡æ¶ˆæ¯ç»„æˆçš„ ChatPromptValue
```

### 1.4 Chaining with LCEL- é“¾æ¥

ä½¿ç”¨ç«–çº¿ ï¼ˆ'|'ï¼‰ è¿ç®—ç¬¦å°†å…¶ä¸ä¸Šé¢çš„æ¨¡å‹å’Œè¾“å‡ºè§£æå™¨ç›¸ç»“åˆï¼š

```python
chain = prompt_template | model | parser
chain.invoke({"language": "chinese", "text": "hi"})
```

è¿™ç§æ–¹æ³•æœ‰å‡ ä¸ªå¥½å¤„ï¼ŒåŒ…æ‹¬ä¼˜åŒ–çš„æµå¼å¤„ç†å’Œè·Ÿè¸ªæ”¯æŒ.

### 1.5 Serving with LangServe - æ„å»ºåº”ç”¨ç¨‹åº

[ğŸ¦œï¸LangServe](https://python.langchain.com/docs/langserve/)

ä½¿ç”¨ LangServe éƒ¨ç½²åº”ç”¨ç¨‹åºï¼Œæ•ˆæœå¦‚ä¸‹ï¼š

å®‰è£…ç¯å¢ƒ `pip install "langserve[all]"`

##### tp-image-playground

<img src="./asstes/LangChain-Rag/image-20241104182536316.png" alt="image-20241104182536316" style="zoom: 67%;" />

æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªserve.py,å®ƒå°†åŒ…å«ä¸‰éƒ¨åˆ†

- æˆ‘ä»¬ä¸Šé¢åˆšåˆšæ„å»ºçš„é“¾çš„å®šä¹‰
- FastAPI app
- ä¸ºé“¾æä¾›æœåŠ¡çš„è·¯ç”±çš„å®šä¹‰ï¼Œè¯¥å®šä¹‰ç”± `langserve.add_routes` å®Œæˆ

**serve.py**

```python
#!/usr/bin/env python
from fastapi import FastAPI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langserve import add_routes

# 1. Create prompt template
system_template = "Translate the following into {language}:"
prompt_template = ChatPromptTemplate.from_messages([
    ('system', system_template),
    ('user', '{text}')
])

# 2. Create model
model = ChatOpenAI(model="gpt-3.5-turbo",api_key="sk-xxxx", base_url="https://api.bianxie.ai/v1")

# 3. Create parser
parser = StrOutputParser()

# 4. Create chain
chain = prompt_template | model | parser

# 5. App definition
app = FastAPI(
  title="LangChain Server",
  version="1.0",
  description="A simple API server using LangChain's Runnable interfaces",
)

# 6. Adding chain route
add_routes(
    app,
    chain,
    path="/chain",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

æ‰§è¡Œè¯¥pyç¨‹åºåï¼ŒæœåŠ¡å°†è¢«å¼€å¯åœ¨ [http://localhost:8000](http://localhost:8000/).

æ­¤æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨http://localhost:8000/chain/playground/ä½¿ç”¨å…·æœ‰**UIç•Œé¢**çš„å›ç­”çª—å£

#### å®¢æˆ·ç«¯ï¼š

ç°åœ¨ï¼Œè®©æˆ‘ä»¬è®¾ç½®ä¸€ä¸ªå®¢æˆ·ç«¯ï¼Œä»¥ä¾¿ä»¥ç¼–ç¨‹æ–¹å¼ä¸æˆ‘ä»¬çš„æœåŠ¡äº¤äº’ã€‚ä½¿ç”¨`langserve.RemoteRunnable`å¯ä»¥ä¸è¢«æœåŠ¡çš„é“¾è¿›è¡Œäº¤äº’ï¼Œå°±åƒå®ƒåœ¨å®¢æˆ·ç«¯è¿è¡Œä¸€æ ·ã€‚

```python
from langserve import RemoteRunnable

remote_chain = RemoteRunnable("http://localhost:8000/chain/")
remote_chain.invoke({"language": "English", "text": "Are You GOOD?"})
```

## 2 åœ¨æœ¬åœ°è¿è¡ŒLLM

### 2.1 Ollama

`pip install langchain_ollama`

```python
from langchain_ollama import OllamaLLM

llm = OllamaLLM(model="llama3.2:latest")
print(llm.invoke("é‡‘æ‘‡æ†å¥–ç¬¬2åæ˜¯ï¼Ÿ"))
```

##  --- --- --- ---

## A1 åŠ è½½å™¨

- ä»ç›®å½•åŠ è½½æ•°æ®
- åŠ è½½PDFæ–‡ä»¶

### A1.1 ä»ç›®å½•åŠ è½½æ•°æ®

`DirectoryLoader` æ¥å—ä¸€ä¸ª `loader_cls` å…³é”®å­—å‚æ•°ï¼Œé»˜è®¤ä¸º [UnstructuredLoader](https://python.langchain.ac.cn/docs/integrations/document_loaders/unstructured_file/),å®ƒæ”¯æŒè§£æå¤šç§æ ¼å¼ï¼Œä¾‹å¦‚ PDF å’Œ HTML

```python
from langchain_community.document_loaders import DirectoryLoader

# å¯ä»¥ä½¿ç”¨ glob å‚æ•°æ¥æ§åˆ¶è¦åŠ è½½çš„æ–‡ä»¶ã€‚è¿™é‡Œå®ƒä¸ä¼šåŠ è½½ .rst æ–‡ä»¶æˆ– .html æ–‡ä»¶
loader = DirectoryLoader("../data", glob="**/*.md") # å¯ä»¥æ·»åŠ å‚æ•°show_progress=Trueï¼Œæ¥å¼€å¯è¿›åº¦æ¡; å‚æ•°use_multithreading=Trueï¼Œæ¥ä½¿ç”¨å¤šçº¿ç¨‹; silent_errors=True å¼€å¯é™é»˜å¤±è´¥ï¼Œè·³è¿‡æ— æ³•åŠ è½½çš„æ–‡ä»¶
docs = loader.load()
print(len(docs))
```

### A1.2 TextLoader

æ³¨æ„ï¼šè™½ç„¶`UnstructuredLoader`è§£æMarkdownæ ‡é¢˜ï¼Œä½†`TextLoader`ä¸ä¼šï¼Œè€Œä¸”**ä½¿ç”¨ä¸‹é¢çš„ä»£ç åŠ è½½txtæ–‡ä»¶ä¼šæŠ¥é”™**

```python
from langchain_community.document_loaders import TextLoader

# è¿™é‡Œä¹Ÿæ˜¯åŠ è½½ä¸€ä¸ªæ–‡ä»¶å¤¹,åŠ è½½å…¶ä¸­çš„.mdæ–‡ä»¶
loader = DirectoryLoader("../", glob="**/*.md", loader_cls=TextLoader)
docs = loader.load()
print(docs[0].page_content[:100])
```

#### ä½¿ç”¨TextLoaderè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç 

```python
text_loader_kwargs = {"autodetect_encoding": True}
loader = DirectoryLoader(
    "../data", glob="**/*.txt", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs
)
docs = loader.load()
print(docs[0].page_content[:100])
>>> # æ­£å¸¸è¯»å–txtæ–‡ä»¶
é‡‘æ‘‡æ†å¥– X GamesRadar è¯„50æ¬¾å²ä¸Šæœ€æœ‰å½±å“åŠ›çš„æ¸¸æˆ:
50 é›†åˆå•¦åŠ¨ç‰©ä¹‹æ£®ï¼ˆ2020ï¼‰
49 åˆºå®¢ä¿¡æ¡ å¥¥å¾·èµ›ï¼ˆ2020ï¼‰
48 Fez ï¼ˆ2021ï¼‰
```



## A2 æ–‡æœ¬åˆ†å‰²å™¨

#### åŠ è½½æ•°æ®

```python
with open("../data/XgameRadar.txt", 'r', encoding='utf-8') as f:
    state_of_the_union = f.read() # state_of_the_unionæ˜¯strç±»å‹æ•°æ®
```

#### åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

# ä½¿ç”¨OpenAI
#text_splitter = SemanticChunker(OpenAIEmbeddings(api_key="sk-xxxx",base_url="https://api.bianxie.ai/v1"))
# ä½¿ç”¨Ollama,# ç¬¬ä¸€ä¸ªå‚æ•°ï¼ŒEmbeddingæ¨¡å‹; ç¬¬äºŒä¸ªå‚æ•°breakpoint_threshold_type
text_splitter = SemanticChunker(OllamaEmbeddings(model="bge-large:latest"),
                                breakpoint_threshold_type="gradient")

```

#### åˆ†å‰²æ–‡æœ¬

â€‹	æˆ‘ä»¬ä»¥é€šå¸¸çš„æ–¹å¼åˆ†å‰²æ–‡æœ¬ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡è°ƒç”¨ `.create_documents` æ¥åˆ›å»º LangChain Documentå¯¹è±¡

```python
docs = text_splitter.create_documents([state_of_the_union])
print(docs[0].page_content)
```

#### æ–­ç‚¹

æ­¤åˆ†å—å™¨é€šè¿‡ç¡®å®šä½•æ—¶â€œåˆ†å‰²â€å¥å­æ¥å·¥ä½œã€‚è¿™æ˜¯é€šè¿‡æŸ¥çœ‹ä»»æ„ä¸¤ä¸ªå¥å­ä¹‹é—´åµŒå…¥çš„å·®å¼‚æ¥å®Œæˆçš„ã€‚å½“è¯¥å·®å¼‚è¶…è¿‡æŸä¸ªé˜ˆå€¼æ—¶ï¼Œå°±ä¼šå°†å…¶åˆ†å‰²ã€‚

æœ‰å‡ ç§æ–¹æ³•å¯ä»¥ç¡®å®šè¯¥é˜ˆå€¼ï¼Œè¿™äº›æ–¹æ³•ç”± `breakpoint_threshold_type` å…³é”®å­—å‚æ•°æ§åˆ¶ã€‚

ä¸‹åˆ—æ˜¯å‡ ä¸ªå…³é”®å­—å‚æ•°

- percentile - ç™¾åˆ†ä½æ•°
- standard_deviation - æ ‡å‡†å·®
- interquartile - å››åˆ†ä½è·
- gradient - æ¢¯åº¦

## A3 åµŒå…¥æ¨¡å‹

[åµŒå…¥æ¨¡å‹](https://python.langchain.ac.cn/docs/concepts/#embedding-models)

- [å¦‚ä½•ï¼šåµŒå…¥æ–‡æœ¬æ•°æ®](https://python.langchain.ac.cn/docs/how_to/embed_text/)
- [å¦‚ä½•ï¼šç¼“å­˜åµŒå…¥ç»“æœ](https://python.langchain.ac.cn/docs/how_to/caching_embeddings/)

## A4 å‘é‡å­˜å‚¨

[å¦‚ä½•åˆ›å»ºå’ŒæŸ¥è¯¢å‘é‡æ•°æ®åº“ | ğŸ¦œï¸ğŸ”— LangChain ä¸­æ–‡](https://python.langchain.ac.cn/docs/how_to/vectorstores/)

åœ¨ä½¿ç”¨å‘é‡æ•°æ®åº“ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åŠ è½½ä¸€äº›æ•°æ®å¹¶åˆå§‹åŒ–ä¸€ä¸ªåµŒå…¥æ¨¡å‹ã€‚

```python
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_text_splitters import CharacterTextSplitter

text_loader_kwargs = {"autodetect_encoding": True}
loader = DirectoryLoader(
    "../data", glob="**/*.txt", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs
)
raw_documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
```

ä½¿ç”¨ `chroma` å‘é‡æ•°æ®åº“

```python
from langchain_chroma import Chroma

db = Chroma.from_documents(documents, OpenAIEmbeddings())
```

### A4.1 ç›¸ä¼¼æ€§æœç´¢

â€‹	æ‰€æœ‰å‘é‡æ•°æ®åº“éƒ½å…¬å¼€äº†ä¸€ä¸ª `similarity_search` æ–¹æ³•ã€‚æ­¤æ–¹æ³•å°†æ¥æ”¶ä¼ å…¥çš„æ–‡æ¡£ï¼Œåˆ›å»ºå®ƒä»¬çš„åµŒå…¥ï¼Œç„¶åæ‰¾åˆ°æ‰€æœ‰å…·æœ‰æœ€ç›¸ä¼¼åµŒå…¥çš„æ–‡æ¡£ã€‚

```python
query = "é‡‘æ‘‡æ†å¥–"
docs = db.similarity_search(query)
print(docs[0].page_content)
```

#### åŸºäºå‘é‡çš„ç›¸ä¼¼æ€§æœç´¢

â€‹	è¿˜å¯ä»¥ä½¿ç”¨ `similarity_search_by_vector` æœç´¢ä¸ç»™å®šåµŒå…¥å‘é‡ç›¸ä¼¼çš„æ–‡æ¡£ï¼Œè¯¥æ–¹æ³•æ¥å—åµŒå…¥å‘é‡ä½œä¸ºå‚æ•°è€Œä¸æ˜¯å­—ç¬¦ä¸²ã€‚

```python
embedding_vector = OllamaEmbeddings(model="bge-large:latest").embed_query(query)
docs = db.similarity_search_by_vector(embedding_vector)
print(docs[0].page_content)
```

**å®Œæ•´ä»£ç ï¼š**

### A4.1 å¼‚æ­¥æ“ä½œ

â€‹	[ç›¸ä¼¼æ€§æœç´¢ - å¼‚æ­¥æ“ä½œ](https://python.langchain.ac.cn/docs/how_to/vectorstores/#async-operations)



## A5 æ£€ç´¢å™¨

æ£€ç´¢å™¨æ˜¯ä¸€ä¸ªæ¥å£ï¼Œå®ƒæ ¹æ®éç»“æ„åŒ–æŸ¥è¯¢è¿”å›æ–‡æ¡£ã€‚å®ƒæ¯”å‘é‡å­˜å‚¨æ›´é€šç”¨ã€‚æ£€ç´¢å™¨ä¸éœ€è¦èƒ½å¤Ÿå­˜å‚¨æ–‡æ¡£ï¼Œåªéœ€è¦è¿”å›ï¼ˆæˆ–æ£€ç´¢ï¼‰å®ƒä»¬å³å¯ã€‚æ£€ç´¢å™¨å¯ä»¥ä»å‘é‡å­˜å‚¨åˆ›å»ºï¼Œä½†ä¹Ÿè¶³å¤Ÿå¹¿æ³›ï¼Œå¯ä»¥åŒ…æ‹¬[ç»´åŸºç™¾ç§‘æœç´¢](https://python.langchain.ac.cn/docs/integrations/retrievers/wikipedia/)å’Œ[Amazon Kendra](https://python.langchain.ac.cn/docs/integrations/retrievers/amazon_kendra_retriever/)ã€‚

æ£€ç´¢å™¨ä»¥å­—ç¬¦ä¸²æŸ¥è¯¢ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»¥ Document åˆ—è¡¨ä½œä¸ºè¾“å‡ºã€‚

### A5.1 ä½¿ç”¨å‘é‡å­˜å‚¨æ£€ç´¢æ•°æ®

å®ƒæ˜¯åœ¨å‘é‡æ•°æ®åº“ç±»å‘¨å›´çš„ä¸€ä¸ªè½»é‡çº§åŒ…è£…å™¨ï¼Œä½¿å…¶ç¬¦åˆæ£€ç´¢å™¨æ¥å£ã€‚å®ƒä½¿ç”¨å‘é‡æ•°æ®åº“ä¸­å®ç°çš„æœç´¢æ–¹æ³•ï¼Œå¦‚ç›¸ä¼¼æ€§æœç´¢å’Œ MMRï¼Œæ¥æŸ¥è¯¢å‘é‡æ•°æ®åº“ä¸­çš„æ–‡æœ¬ã€‚

- å¦‚ä½•ä»å‘é‡æ•°æ®åº“å®ä¾‹åŒ–ä¸€ä¸ªæ£€ç´¢å™¨ï¼›
- å¦‚ä½•ä¸ºæ£€ç´¢å™¨æŒ‡å®šæœç´¢ç±»å‹ï¼›
- å¦‚ä½•æŒ‡å®šå…¶ä»–æœç´¢å‚æ•°ï¼Œä¾‹å¦‚é˜ˆå€¼åˆ†æ•°å’Œ top-k

#### A5.1.1 ä»å‘é‡æ•°æ®åº“åˆ›å»ºæ£€ç´¢å™¨

ä½¿ç”¨å‘é‡æ•°æ®åº“çš„ `.as_retriever` æ–¹æ³•ä»å‘é‡æ•°æ®åº“æ„å»ºä¸€ä¸ªæ£€ç´¢å™¨

```python
# é¦–å…ˆï¼Œæˆ‘ä»¬å®ä¾‹åŒ–ä¸€ä¸ªå‘é‡æ•°æ®åº“ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå†…å­˜ä¸­çš„ FAISS å‘é‡æ•°æ®åº“
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter

loader = TextLoader("state_of_the_union.txt")

documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(texts, embeddings)
```

ç„¶åæˆ‘ä»¬å¯ä»¥**å®ä¾‹åŒ–ä¸€ä¸ªæ£€ç´¢å™¨**

```python
retriever = vectorstore.as_retriever()
#è¿™ä¼šåˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨ï¼ˆå…·ä½“æ¥è¯´æ˜¯ VectorStoreRetrieverï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·ä½¿ç”¨å®ƒ
docs = retriever.invoke("é‡‘æ‘‡æ†å¥–ç¬¬2åæ˜¯?")
```

#### A5.1.2 æœ€å¤§è¾¹é™…ç›¸å…³æ€§æ£€ç´¢

é»˜è®¤æƒ…å†µä¸‹ï¼Œå‘é‡æ•°æ®åº“æ£€ç´¢å™¨ä½¿ç”¨ç›¸ä¼¼æ€§æœç´¢ã€‚**å¦‚æœåº•å±‚å‘é‡æ•°æ®åº“æ”¯æŒæœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢**ï¼Œåˆ™å¯ä»¥å°†å…¶æŒ‡å®šä¸ºæœç´¢ç±»å‹

è¿™å®é™…ä¸ŠæŒ‡å®šäº†ä½¿ç”¨åº•å±‚å‘é‡æ•°æ®åº“ä¸Šçš„ä»€ä¹ˆæ–¹æ³•`similarity_search`ã€`max_marginal_relevance_search` ç­‰

```python
retriever = vectorstore.as_retriever(search_type="mmr")
docs = retriever.invoke("é‡‘æ‘‡æ†å¥–ç¬¬2åæ˜¯?")
```

### A5.2 ä¼ é€’æœç´¢å‚æ•°

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`search_kwargs`å°†å‚æ•°ä¼ é€’ç»™åº•å±‚å‘é‡å­˜å‚¨çš„æœç´¢æ–¹æ³•

#### A5.2.1 ç›¸ä¼¼åº¦å¾—åˆ†é˜ˆå€¼æ£€ç´¢

æˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ªç›¸ä¼¼åº¦å¾—åˆ†é˜ˆå€¼ï¼Œå¹¶ä¸”**åªè¿”å›å¾—åˆ†é«˜äºè¯¥é˜ˆå€¼çš„æ–‡ä»¶**

```python
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.5}
)
docs = retriever.invoke("é‡‘æ‘‡æ†å¥–ç¬¬2åæ˜¯?")
```

#### A5.2.2 æŒ‡å®štop k

æˆ‘ä»¬è¿˜å¯ä»¥**é™åˆ¶æ£€ç´¢å™¨è¿”å›çš„æ–‡ä»¶æ•°é‡** `k`

```python
retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
docs = retriever.invoke("é‡‘æ‘‡æ†å¥–ç¬¬2åæ˜¯?")
len(docs)
```



##  --- --- --- ---

## x0 ç¯å¢ƒå‡†å¤‡

### x0.0 pythonç¯å¢ƒ

```powershell
 pip install langchain langchain-community langchain-core langchain-cli
 pip install openai
 pip install "langserve[all]"
 
 # åŠ è½½å™¨
 pip install unstructured unstructured[md]
 pip install python-magic-bin
 
 # åˆ†å‰²å™¨
 pip install langchain_experimental langchain_openai
 
 # å‘é‡å­˜å‚¨
 pip install langchain-chroma
 
```

### x0.1 LangChainç¯å¢ƒé…ç½®

#### x0.1.1 

```
import getpass
import os

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
```

#### x0.1.2

```
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY="..."
```

## x1 æ¥å£æ–‡æ¡£

- [æ–‡æœ¬åŠ è½½å™¨](https://python.langchain.ac.cn/docs/how_to/#document-loaders)
  - [document_loaders](https://python.langchain.com/api_reference/community/document_loaders.html)
- [æ–‡æœ¬åˆ†å‰²å™¨](https://python.langchain.ac.cn/docs/concepts/#text-splitters)
  - [åˆ†å‰²å™¨ä½¿ç”¨æŒ‡å—](https://python.langchain.ac.cn/docs/how_to/#text-splitters)
- [æ£€ç´¢å™¨](https://python.langchain.ac.cn/docs/how_to/#retrievers)
  - [æ£€ç´¢å™¨ä½¿ç”¨æŒ‡å—](https://python.langchain.ac.cn/docs/how_to/#retrievers)



## x2 ä¸€äº›æŠ¥é”™çš„è§£å†³æ–¹æ³•

### x2.1 RuntimeError: Error loading ../data/xxxx.txt

é€šå¸¸å‘ç”Ÿåœ¨è¯»å–txtæ–‡ä»¶æ—¶ï¼Œå‘ç”Ÿè¿™ä¸ªé”™è¯¯çš„åŸå› åœ¨äºTextLoaderä¸çŸ¥é“è¯¥æ–‡ä»¶çš„ç¼–ç ï¼Œå¯ä»¥æŸ¥çœ‹[è§£å†³æ–¹æ³•](#ä½¿ç”¨TextLoaderè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç )
