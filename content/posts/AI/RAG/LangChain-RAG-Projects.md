---
title: 'LangChain Rag Projects - 2.åŸºæœ¬é¡¹ç›®çš„æž„å»º'
date: 2024-11-05T16:48:15+08:00
draft: false
tags: ['AI','LangChain','RAG']
---

# LangChain RAG çš„æ¼”ç¤ºé¡¹ç›®æž„å»º

## 1 æž„å»ºèŠå¤©æœºå™¨äºº (åŒ…å«ä¸Šä¸‹æ–‡)

### 1.1 å¯¼å…¥æ¨¡åž‹

æˆ‘å°†ä»‹ç»å¦‚ä½•è®¾è®¡å’Œå®žçŽ°ä¸€ä¸ªç”± LLM é©±åŠ¨çš„èŠå¤©æœºå™¨äººçš„ç¤ºä¾‹ã€‚è¿™ä¸ªèŠå¤©æœºå™¨äºº**èƒ½å¤Ÿè¿›è¡Œå¯¹è¯å¹¶è®°ä½ä¹‹å‰çš„äº¤äº’**ã€‚

è¿™ä¸ªç¨‹åºå°†ä»¥æœ¬åœ°è¿è¡Œçš„Ollamaæ¨¡åž‹llama3.2:latestä¸ºLLM

```python
from langchain_ollama import OllamaLLM
model = OllamaLLM(model='llama3.2:latest')
```

è®©æˆ‘ä»¬é¦–å…ˆç›´æŽ¥ä½¿ç”¨æ¨¡åž‹ã€‚`ChatModel` æ˜¯ LangChainâ€œå¯è¿è¡Œå¯¹è±¡â€çš„å®žä¾‹ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å…¬å¼€äº†ç”¨äºŽä¸Žå…¶äº¤äº’çš„æ ‡å‡†æŽ¥å£ã€‚è¦ç®€å•åœ°è°ƒç”¨æ¨¡åž‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¶ˆæ¯åˆ—è¡¨ä¼ é€’ç»™ `.invoke` æ–¹æ³•

```python
from langchain_core.messages import HumanMessage
print(model.invoke([HumanMessage(content="ä½ å¥½ï¼Œæˆ‘æ˜¯yyz")]))
```

æ¨¡åž‹æœ¬èº«æ²¡æœ‰ä»»ä½•çŠ¶æ€çš„æ¦‚å¿µï¼Œ**æ­¤æ—¶å®ƒæ²¡æœ‰å°†ä¹‹å‰çš„å¯¹è¯è½®æ¬¡çº³å…¥ä¸Šä¸‹æ–‡**ï¼Œå¦‚æžœå†é—®ä»–æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Œå®ƒæ— æ³•å›žç­”

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¯•ç€å°†æ•´ä¸ªå¯¹è¯åŽ†å²è®°å½•ä¼ é€’ç»™æ¨¡åž‹

```python
from langchain_core.messages import AIMessage
model.invoke(
    [
        HumanMessage(content="ä½ å¥½ï¼Œæˆ‘æ˜¯yyz"),
        AIMessage(content="å¤§å®¶å¥½ï¼ä½ å¥½ï¼Œyyzï¼ (hello, yyz!) æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿä½ æ¥è‡ªå“ªé‡Œï¼Ÿ"),
        HumanMessage(content="What's my name?"),
    ]
)
```

![image-20241105171047686](../asstes/LangChain-RAG-Projects/image-20241105171047686.png)

å¦‚å›¾ï¼Œå®ƒè¡¨ç¤ºå·²ç»çŸ¥é“æˆ‘çš„åå­—äº†ã€‚

### 1.2 æ¶ˆæ¯æŒä¹…åŒ–

å°†æˆ‘ä»¬çš„èŠå¤©æ¨¡åž‹åŒ…è£…åœ¨ä¸€ä¸ªæœ€å°çš„ [LangGraph](https://github.langchain.ac.cn/langgraph/) åº”ç”¨ç¨‹åºä¸­ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿè‡ªåŠ¨æŒä¹…åŒ–æ¶ˆæ¯åŽ†å²è®°å½•ï¼Œç®€åŒ–å¤šè½®åº”ç”¨ç¨‹åºçš„å¼€å‘

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

# å®šä¹‰ä¸€ä¸ªæ–°çš„Graph
workflow = StateGraph(state_schema=MessagesState)

# å®šä¹‰ä¸€ä¸ªå“åº”æ¨¡åž‹çš„å‡½æ•°
def call_model(state: MessagesState):
    response = model.invoke(state["messages"])
    return {"messages": response}

# å®šä¹‰å›¾å½¢ä¸­çš„ï¼ˆå•ä¸ªï¼‰èŠ‚ç‚¹
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# Add memory
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

çŽ°åœ¨æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª `config`ï¼Œæ¯æ¬¡ä¼ é€’ç»™å¯è¿è¡Œå¯¹è±¡æ—¶éƒ½éœ€è¦å®ƒã€‚**æ­¤é…ç½®åŒ…å«ä¸ç›´æŽ¥å±žäºŽè¾“å…¥çš„ä¸€éƒ¨åˆ†ï¼Œä½†ä»ç„¶æœ‰ç”¨çš„ä¿¡æ¯**ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›åŒ…å«ä¸€ä¸ª `thread_id`ã€‚è¿™åº”è¯¥çœ‹èµ·æ¥åƒè¿™æ ·

```python
config = {"configurable": {"thread_id": "foryyz"}}
```

è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨å•ä¸ªåº”ç”¨ç¨‹åºæ”¯æŒå¤šä¸ªå¯¹è¯çº¿ç¨‹ï¼Œè¿™æ˜¯åº”ç”¨ç¨‹åºæœ‰å¤šä¸ªç”¨æˆ·æ—¶çš„å¸¸è§éœ€æ±‚

ç„¶åŽæˆ‘ä»¬å¯ä»¥è°ƒç”¨åº”ç”¨ç¨‹åº

```python
query = "Hi! I'm yyz."
input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()  # output contains all messages in state

query = "What's my name?"
input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

æ­¤æ—¶ï¼Œæˆ‘ä»¬çš„èŠå¤©æœºå™¨äººè®°ä½äº†å…³äºŽæˆ‘ä»¬çš„åå­—ï¼Œå¦‚æžœæ­¤æ—¶å°†configçš„`thread_id`æ›´æ”¹ï¼Œå†é—®ä»–æˆ‘çš„åå­—

```python
# æ›´æ”¹ thread_id
config = {"configurable": {"thread_id": "abc123"}}

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

æ­¤æ—¶æˆ‘ä»¬å‘çŽ°å®ƒé‡æ–°å¼€å¯äº†ä¸€æ®µå¯¹è¯.

![image-20241105212504281](../asstes/LangChain-RAG-Projects/image-20241105212504281.png)

### 1.3 (Can Be Skipped)å¯¹äºŽå¼‚æ­¥çš„æ”¯æŒ

å¯¹äºŽå¼‚æ­¥æ”¯æŒï¼Œè¯·æ›´æ–° `call_model` èŠ‚ç‚¹ä½¿å…¶æˆä¸ºå¼‚æ­¥å‡½æ•°ï¼Œå¹¶åœ¨è°ƒç”¨åº”ç”¨ç¨‹åºæ—¶ä½¿ç”¨ `.ainvoke`

```python
# Async function for node:
async def call_model(state: MessagesState):
    response = await model.ainvoke(state["messages"])
    return {"messages": response}


# Define graph as before:
workflow = StateGraph(state_schema=MessagesState)
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)
app = workflow.compile(checkpointer=MemorySaver())

# Async invocation:
output = await app.ainvoke({"messages": input_messages}, config):
output["messages"][-1].pretty_print()
```

ç›®å‰ï¼Œæˆ‘ä»¬æ‰€åšçš„åªæ˜¯åœ¨æ¨¡åž‹å‘¨å›´æ·»åŠ äº†ä¸€ä¸ªç®€å•çš„æŒä¹…åŒ–å±‚ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ æç¤ºæ¨¡æ¿ä½¿å…¶å˜å¾—æ›´å¤æ‚å’Œä¸ªæ€§åŒ–ã€‚

### 1.4 æç¤ºæ¨¡æ¿

**æç¤ºæ¨¡æ¿**æœ‰åŠ©äºŽ**å°†åŽŸå§‹ç”¨æˆ·ä¿¡æ¯è½¬æ¢ä¸º LLM å¯ä»¥å¤„ç†çš„æ ¼å¼**ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒåŽŸå§‹ç”¨æˆ·è¾“å…¥åªæ˜¯ä¸€æ¡æ¶ˆæ¯ï¼Œæˆ‘ä»¬å°†å…¶ä¼ é€’ç»™ LLMã€‚çŽ°åœ¨è®©æˆ‘ä»¬ä½¿å…¶ç¨å¾®å¤æ‚ä¸€äº›ã€‚é¦–å…ˆï¼Œ**è®©æˆ‘ä»¬æ·»åŠ ä¸€æ¡å¸¦æœ‰è‡ªå®šä¹‰æŒ‡ä»¤çš„ç³»ç»Ÿæ¶ˆæ¯**ï¼ˆä½†ä»ç„¶å°†æ¶ˆæ¯ä½œä¸ºè¾“å…¥ï¼‰ã€‚æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ·»åŠ é™¤æ¶ˆæ¯ä¹‹å¤–çš„æ›´å¤šè¾“å…¥ã€‚

è¦æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª `ChatPromptTemplate`ã€‚æˆ‘ä»¬å°†åˆ©ç”¨ `MessagesPlaceholder` ä¼ é€’æ‰€æœ‰æ¶ˆæ¯ã€‚

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You talk like a pirate. Answer all questions to the best of your ability.",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)
```

çŽ°åœ¨æˆ‘ä»¬å¯ä»¥æ›´æ–°æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºä»¥åˆå¹¶æ­¤æ¨¡æ¿

```python
workflow = StateGraph(state_schema=MessagesState)

def call_model(state: MessagesState):
    chain = prompt | model
    response = chain.invoke(state)
    return {"messages": response}

workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

ä»¥ç›¸åŒçš„æ–¹å¼è°ƒç”¨åº”ç”¨ç¨‹åº

```python
config = {"configurable": {"thread_id": "foryyz1"}}
query = "ä½ å¥½ï¼Œæˆ‘çš„åå­—æ˜¯yyz."

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

æ•ˆæžœå¾ˆå¥½

![image-20241105213804805](../asstes/LangChain-RAG-Projects/image-20241105213804805.png)

åœ¨è®©æˆ‘ä»¬**ä½¿æˆ‘ä»¬çš„æç¤ºç¨å¾®å¤æ‚ä¸€äº›**ã€‚å‡è®¾æç¤ºæ¨¡æ¿çŽ°åœ¨çœ‹èµ·æ¥åƒè¿™æ ·

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å·²å‘æç¤ºæ·»åŠ äº†æ–°çš„ `language` è¾“å…¥ã€‚æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºçŽ°åœ¨æœ‰**ä¸¤ä¸ªå‚æ•°**â€”â€”è¾“å…¥ `messages` å’Œ `language`ã€‚

```python
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. Answer all questions to the best of your ability in {language}.",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)
```

æˆ‘ä»¬æ›´æ–°åº”ç”¨ç¨‹åºçš„çŠ¶æ€ä»¥æ·»åŠ languageå‚æ•°ã€‚[BaseMessage](https://python.langchain.ac.cn/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html)ã€[add_messages](https://github.langchain.ac.cn/langgraph/reference/graphs/#langgraph.graph.message.add_messages)

```python
from typing import Sequence

from langgraph.constants import START
from langgraph.graph import StateGraph
from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict
from langgraph.checkpoint.memory import MemorySaver

class State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    language: str
    
workflow = StateGraph(state_schema=State)

def call_model(state: State):
    chain = prompt | model
    response = chain.invoke(state)
    return {"messages": [response]}

workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

æé—®

```python
config = {"configurable": {"thread_id": "foryyz2"}}
language = "Chinese"

query = "Hi My name is yyz."
input_messages = [HumanMessage(query)]
output = app.invoke(
    {"messages": input_messages, "language": language},
    config,
)
output["messages"][-1].pretty_print()
```

### 1.5 ç®¡ç†ä¸Šä¸‹æ–‡å¤§å° (Tokenä¸Šé™)

åœ¨æž„å»ºèŠå¤©æœºå™¨äººæ—¶ï¼Œéœ€è¦ç†è§£çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µæ˜¯å¦‚ä½•ç®¡ç†å¯¹è¯åŽ†å²è®°å½•ã€‚å¦‚æžœä»»å…¶ä¸å—ç®¡ç†ï¼Œæ¶ˆæ¯åˆ—è¡¨å°†æ— é™å¢žé•¿ï¼Œ**å¹¶å¯èƒ½è¶…å‡º LLM çš„ä¸Šä¸‹æ–‡çª—å£**ã€‚å› æ­¤ï¼Œ**æ·»åŠ ä¸€ä¸ªé™åˆ¶æ‚¨ä¼ é€’çš„æ¶ˆæ¯å¤§å°**çš„æ­¥éª¤éžå¸¸é‡è¦ã€‚

**é‡è¦çš„æ˜¯ï¼Œéœ€è¦åœ¨æç¤ºæ¨¡æ¿ä¹‹å‰ä½†åŠ è½½æ¶ˆæ¯åŽ†å²è®°å½•ä¸­çš„å…ˆå‰æ¶ˆæ¯ä¹‹åŽæ‰§è¡Œæ­¤æ“ä½œã€‚**

æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨æç¤ºå‰é¢æ·»åŠ ä¸€ä¸ªç®€å•çš„æ­¥éª¤æ¥ä¿®æ”¹ `messages` é”®ï¼Œç„¶åŽå°†æ–°çš„é“¾åŒ…è£…åœ¨æ¶ˆæ¯åŽ†å²è®°å½•ç±»ä¸­æ¥å®žçŽ°ã€‚

```python
from langchain_core.messages import SystemMessage, trim_messages
from langchain_openai import ChatOpenAI
trimmer = trim_messages(
    max_tokens=65,
    strategy="last",
    # è¿™é‡Œä½¿ç”¨ç¬¬ä¸‰æ–¹å¤§æ¨¡åž‹è¿›è¡Œtokenç»Ÿè®¡
    token_counter=ChatOpenAI(model="gpt-3.5-turbo",api_key="sk-xxxx", base_url="https://api.bianxie.ai/v1"),
    include_system=True,
    allow_partial=False,
    start_on="human",
)

messages = [
    SystemMessage(content="you're a good assistant"),
    HumanMessage(content="hi! I'm yyz"),
    AIMessage(content="hi!"),
    HumanMessage(content="I like vanilla ice cream"),
    AIMessage(content="nice"),
    HumanMessage(content="whats 2 + 2"),
    AIMessage(content="4"),
    HumanMessage(content="thanks"),
    AIMessage(content="no problem!"),
    HumanMessage(content="having fun?"),
    AIMessage(content="yes!"),
]

print(trimmer.invoke(messages))
```

å¯¹äºŽä¿®å‰ªå™¨çš„æ›´é«˜çº§çš„ä½¿ç”¨ï¼š[å¦‚ä½•ä¿®å‰ªæ¶ˆæ¯ | ðŸ¦œï¸ðŸ”— LangChain ä¸­æ–‡](https://python.langchain.ac.cn/docs/how_to/trim_messages/)

è¦åœ¨æˆ‘ä»¬çš„é“¾ä¸­ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨å°† `messages` è¾“å…¥ä¼ é€’ç»™æˆ‘ä»¬çš„æç¤ºä¹‹å‰è¿è¡Œä¿®å‰ªå™¨å³å¯ã€‚

```python
workflow = StateGraph(state_schema=State)

def call_model(state: State):
    chain = prompt | model
    trimmed_messages = trimmer.invoke(state["messages"])
    response = chain.invoke(
        {"messages": trimmed_messages, "language": state["language"]}
    )
    return {"messages": [response]}

workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

æˆ‘ä»¬é—®ä»–ä¸¤ä¸ªåŒ…å«åœ¨åŽ†å²é—®ç­”ä¸­çš„é—®é¢˜è¿›è¡Œæµ‹è¯•

```python
# çŽ°åœ¨ï¼Œå¦‚æžœæˆ‘ä»¬å°è¯•è¯¢é—®æ¨¡åž‹æˆ‘ä»¬çš„å§“åï¼Œå®ƒå°†ä¸çŸ¥é“ï¼Œå› ä¸ºæˆ‘ä»¬ä¿®å‰ªäº†èŠå¤©åŽ†å²è®°å½•çš„é‚£ä¸€éƒ¨åˆ†
config = {"configurable": {"thread_id": "foryyz2"}}
query = "What is my name?"
language = "English"

input_messages = messages + [HumanMessage(query)]
output = app.invoke(
    {"messages": input_messages, "language": language},
    config,
)
output["messages"][-1].pretty_print()

# ä½†æ˜¯ï¼Œå¦‚æžœæˆ‘ä»¬è¯¢é—®æœ€è¿‘å‡ æ¡æ¶ˆæ¯ä¸­çš„ä¿¡æ¯ï¼Œå®ƒä¼šè®°ä½
config = {"configurable": {"thread_id": "abc678"}}
query = "What math problem did I ask?"
language = "English"

input_messages = messages + [HumanMessage(query)]
output = app.invoke(
    {"messages": input_messages, "language": language},
    config,
)
output["messages"][-1].pretty_print()
```

å¦‚å›¾ï¼š

![image-20241105221656297](../asstes/LangChain-RAG-Projects/image-20241105221656297.png)

### 1.6 æµå¼ä¼ è¾“

çŽ°åœ¨æˆ‘ä»¬å·²ç»æ‹¥æœ‰äº†ä¸€ä¸ªåŠŸèƒ½æ­£å¸¸çš„èŠå¤©æœºå™¨äºº,ä½†æ˜¯ LLM æœ‰æ—¶å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´æ‰èƒ½åšå‡ºå“åº”ï¼Œå› æ­¤ä¸ºäº†æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œå¤§å¤šæ•°åº”ç”¨ç¨‹åºéƒ½ä¼šåšçš„ä¸€ä»¶äº‹æ˜¯ï¼Œåœ¨ç”Ÿæˆæ¯ä¸ª token æ—¶å°†å…¶æµå¼ä¼ è¾“å›žã€‚è¿™å…è®¸ç”¨æˆ·çœ‹åˆ°è¿›åº¦ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ LangGraph åº”ç”¨ç¨‹åºä¸­çš„ `.stream` ä¼šæµå¼ä¼ è¾“åº”ç”¨ç¨‹åºæ­¥éª¤â€”â€”åœ¨æœ¬ä¾‹ä¸­ï¼Œæ˜¯æ¨¡åž‹å“åº”çš„å•ä¸ªæ­¥éª¤ã€‚å°† `stream_mode="messages"` è®¾ç½®ä¸ºå…è®¸æˆ‘ä»¬æ”¹ä¸ºæµå¼ä¼ è¾“è¾“å‡º token

```python
config = {"configurable": {"thread_id": "abc789"}}
query = "Hi I'm yyz, please tell me a joke."
language = "English"

input_messages = [HumanMessage(query)]
for chunk, metadata in app.stream(
    {"messages": input_messages, "language": language},
    config,
    stream_mode="messages",
):
    if isinstance(chunk, AIMessage):  # Filter to just model responses
        print(chunk.content, end="|")
```

### 1.7 å®Œæ•´ç¨‹åº

[LangChain_Basic_Exercise/P1_LangChainBasicChatBot_UseOllamaAndChatGPT.py](https://github.com/foryyz/Programming-Basic-Projects/blob/main/LangChain_Basic_Exercise/P1_LangChainBasicChatBot_UseOllamaAndChatGPT.py)



## x0 çŽ¯å¢ƒå‡†å¤‡

åœ¨å¼€å§‹ä¹‹å‰ï¼Œä½ éœ€è¦å‡†å¤‡å¥½pythonçŽ¯å¢ƒ â†“

[LangChain Rag | ðŸ«¨ Here is yyz!](https://foryyz.top/posts/ai-rag/langchain-rag-basic/#x00-pythonçŽ¯å¢ƒ)

é“¾æŽ¥é‡Œçš„å®‰è£…å¥½ è¿˜æœ‰è¿™äº›ï¼š

```powershell
pip install langchain-core langgraph>0.2.27

pip install beautifulsoup4

pip install langchain-nomic # æ£€ç´¢æ¨¡åž‹
pip install scikit-learn
pip install "nomic[local]"

pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

